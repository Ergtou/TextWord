{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.848 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jieba\n",
    "\n",
    "class_set = set()\n",
    "def trans_data(data_name):\n",
    "    with open(data_name+\".json\",\"r\",encoding=\"utf-8\") as f:\n",
    "        file = f.readlines()\n",
    "        data_dict =[json.loads(ins) for ins in file]\n",
    "    if data_name==\"test\":\n",
    "        with open(data_name+\".txt\",\"w\",encoding = \"utf-8\") as f:\n",
    "            for ins in data_dict:\n",
    "                try:\n",
    "                    label = ins['id']\n",
    "                    seq = ins['sentence']\n",
    "                    keys = ins['keywords'].replace(\",\",\"\")\n",
    "                    fenci_list = jieba.cut(keys+seq)\n",
    "                    seq = \" \".join(fenci_list)\n",
    "                    f.write(seq.strip()+\"\\t\"+str(label)+\"\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "    else:\n",
    "        with open(data_name+\".txt\",\"w\",encoding = \"utf-8\") as f:\n",
    "            for ins in data_dict:\n",
    "                try:\n",
    "                    label = ins['label']\n",
    "                    seq = ins['sentence']\n",
    "                    keys = ins['keywords'].replace(\",\",\"\")\n",
    "                    class_set.add(label)\n",
    "                    fenci_list = jieba.cut(keys+seq)\n",
    "                    seq = \" \".join(fenci_list)\n",
    "                    f.write(seq.strip()+\"\\t\"+str(list(class_set).index(label))+\"\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "trans_data(\"train\")\n",
    "trans_data(\"dev\")\n",
    "trans_data(\"test\")\n",
    "with open(\"class.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for name in class_set:\n",
    "        f.write(name+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92071\n"
     ]
    }
   ],
   "source": [
    "word_set = list()\n",
    "with open(\"traindata.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    while True:\n",
    "    #for _ in range(1):\n",
    "        line = f.readline()\n",
    "        if line ==\"\":\n",
    "            break\n",
    "        text = line.split(\"\\t\")[1]\n",
    "        word_set.extend(text.split())\n",
    "word_set = set(word_set)\n",
    "print(len(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "机械 -0.303912 0.068213 -0.555701 0.126463 0.081653 -0.119685 -0.164275 -0.269583 -0.053705 -0.485537 0.578365 -0.084229 0.252602 -0.448185 0.031737 -0.189915 0.070474 -0.195648 0.200982 0.094391 -0.274643 0.002498 0.058703 -0.337906 0.017945 0.086547 0.638313 0.052043 0.495174 0.184797 0.532837 0.238706 -0.474381 0.097568 -0.426208 -0.014915 -0.001040 -0.373978 -0.482741 0.193526 0.372173 -0.037625 -0.517868 0.370093 -0.045706 0.083684 -0.151832 -0.022427 -0.143722 0.024304 -0.052603 -0.215703 -0.432142 -0.628290 -0.431709 -0.050816 0.192846 0.239857 -0.026663 -0.118478 0.182847 0.270209 -0.171950 0.148767 0.124844 -0.161069 0.402792 -0.109613 -0.505291 -0.160908 0.266840 0.140635 0.141091 0.390437 -0.156587 0.457715 -0.053749 -0.571963 0.233555 0.150497 -0.010233 -0.071188 0.099090 -0.037408 0.466187 0.234833 -0.179498 -0.162159 -0.307508 -0.223744 -0.106901 0.123715 0.357582 -0.256782 0.187605 0.071848 0.034141 0.024912 -0.372561 -0.267724 -0.328495 -0.002208 -0.195250 -0.314027 0.013873 -0.060984 0.630763 -0.141093 0.294239 -0.875204 0.052467 -0.242076 0.107025 -0.189722 -0.086084 0.312213 0.307453 -0.069397 0.483725 0.082497 0.615404 -0.079236 -0.311009 0.156265 0.327645 -0.562772 0.290369 -0.054131 0.552436 -0.174754 -0.245880 -0.327890 -0.208965 0.290863 0.141369 0.323072 -0.279218 0.080268 0.037382 -0.208281 -0.079640 0.561774 0.433899 0.480416 -0.086414 0.242979 0.245392 -0.067080 -0.048362 -0.083473 0.274765 0.204897 -0.392586 -0.062391 -0.047043 0.274157 0.089522 -0.216575 -0.099622 0.279984 0.056869 -0.297894 -0.005373 0.543197 0.709410 0.330415 0.262379 -0.383687 0.006036 -0.976125 0.441832 -0.181435 -0.108460 0.341609 -0.619857 -0.326346 -0.385206 -0.121325 -0.405828 -0.166154 -0.177210 0.176332 0.046160 -0.117224 -0.094444 -0.067654 0.034651 0.161423 0.184318 -0.128674 0.300266 -0.390212 0.194306 0.032220 -0.016073 -0.191167 0.113171 0.138078 -0.316618 -0.274459 -0.024273 0.586775 0.288993 0.138135 0.065620 0.262608 -0.262169 0.032034 -0.302260 0.500701 0.232175 0.050375 0.215284 0.337272 0.540616 -0.153766 0.419054 -0.180765 0.312397 -0.241972 -0.018472 0.457376 0.496227 0.527844 -0.038539 0.272987 0.161449 0.223312 -0.296526 -0.126958 -0.101823 0.264088 0.066618 -0.148118 -0.174426 0.024793 -0.355717 -0.204623 -0.533677 0.410462 0.137827 -0.113557 0.156208 -0.115845 0.040822 0.241481 0.590077 -0.318017 -0.019662 -0.261859 -0.006741 0.395107 -0.432348 0.285377 -0.040058 -0.361884 0.031096 0.278366 -0.237499 0.341503 -0.086090 -0.100921 -0.062213 -0.413112 0.253638 0.318766 -0.380718 0.258367 -0.152552 -0.189792 -0.080406 0.101543 0.007453 -0.311983 0.582248 0.128142 -0.659628 0.287093 -0.057049 0.131950 -0.019263 -0.009168 -0.556533 0.083799 -0.004592 -0.323121 -0.046364 0.148306 0.013235 -0.103777 0.526989 -0.173421 0.396249 0.060519 0.110885 -0.008451 -0.354246 -0.072069 -0.201890 -0.473496 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./sgns.sogou.word\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for i in range(3000):\n",
    "        line = f.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.848 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jieba\n",
    "\n",
    "class_set = set()\n",
    "def trans_data(data_name):\n",
    "    with open(data_name+\".json\",\"r\",encoding=\"utf-8\") as f:\n",
    "        file = f.readlines()\n",
    "        data_dict =[json.loads(ins) for ins in file]\n",
    "    if data_name==\"test\":\n",
    "        with open(data_name+\".txt\",\"w\",encoding = \"utf-8\") as f:\n",
    "            for ins in data_dict:\n",
    "                try:\n",
    "                    label = ins['id']\n",
    "                    seq = ins['sentence']\n",
    "                    #keys = ins['keywords'].replace(\",\",\"\")\n",
    "                    fenci_list = jieba.cut(seq)\n",
    "                    seq = \" \".join(fenci_list)\n",
    "                    f.write(seq.strip()+\"\\t\"+str(label)+\"\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "    else:\n",
    "        with open(data_name+\".txt\",\"w\",encoding = \"utf-8\") as f:\n",
    "            for ins in data_dict:\n",
    "                try:\n",
    "                    label = ins['label']\n",
    "                    seq = ins['sentence']\n",
    "                    #keys = ins['keywords'].replace(\",\",\"\")\n",
    "                    class_set.add(label)\n",
    "                    fenci_list = jieba.cut(seq)\n",
    "                    seq = \" \".join(fenci_list)\n",
    "                    f.write(seq.strip()+\"\\t\"+str(label)+\"\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "trans_data(\"train\")\n",
    "trans_data(\"dev\")\n",
    "trans_data(\"test\")\n",
    "with open(\"class.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for name in class_set:\n",
    "        f.write(name+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
